{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBnrSDPk8u6C"
      },
      "source": [
        "# **Logistic Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKfJyI0R8q3e"
      },
      "source": [
        "## **Gradient Descent**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-ThKrcWEjaR"
      },
      "source": [
        "### **Problem with our approach so far:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFP0HJ3a87P7"
      },
      "source": [
        "Ordinary Least Squares (OLS) method is trying to find **the optimal parameters that minimize the sum of squared errors**. In previous session we obtained a formula (closed-form solution) that allowed us to find the parameters: $\\boldsymbol{\\beta} =(\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{y}$. Put values of given data points and you get solution in a single step. This method is a non-iterative approach for optimization. However, this method assumes:\n",
        "  - Number of samples is larger than the number of independent variables.\n",
        "  - Inverse of $X^TX$ exists.  \n",
        "\n",
        "In addition, the inverse operation, $(X^TX)^{-1}$ is expensive to compute with time increasing very fast with the increase in number of data points. So for a large dataset with a large number of features, OLS can quickly become computationally infeasible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCrSocLbEnJd"
      },
      "source": [
        "### **Enter gradient descent:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fX9e5D1a_su4"
      },
      "source": [
        "Let's start with an example. Suppose you are stuck in the middle of a hill and you want to get to the base of the hill but its all foggy and you can't see a thing. What would you do?\n",
        "\n",
        "Well, one way is to begin by feeling the ground around you and take steps down in the steepest direction and hope you reach towards the bottom.\n",
        "\n",
        "This is exactly what gradient descent method for minimization does."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcUqARXTFRps"
      },
      "source": [
        "### **Gradient Descent Algorithmn**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frI6kD0HFo6I"
      },
      "source": [
        "Consider a real-valued function  $y = f(\\mathbf{x})$ for which you want to find value of $\\mathbf{x}$ that produces the smallest possible output $y$. The gradient descent algorithm works in the following steps:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMa9aCkWFmB3"
      },
      "source": [
        "Step 1: Initialize the value of ${x}$ randomly\n",
        "\n",
        "Step 2: Calculate the gradient of $f({x})$ with respect to ${x}$ ie.  $\\frac{\\partial\\ f({x})}{\\partial\\ {x}}$\n",
        "\n",
        "Step 3: Update ${x}$ as:\n",
        "\n",
        "$${x} = {x} - \\alpha \\frac{\\partial\\ f({x})}{\\partial\\ {x}}$$\n",
        "\n",
        "$\\hspace{10cm}$ here, $\\alpha$ is called the **learning rate**\n",
        "\n",
        "Step 4: Repeat steps 1, 2 and 3 until the value of $f({x})$ converges to the minimum value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLKboAWxHAtd"
      },
      "source": [
        "### **Assignment 1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qigziS19HIMA"
      },
      "source": [
        "Find the minimum value for $y = (x - 6)^2 + sin(3x)$ using gradient descent\n",
        "- Plot the graph of the function and the current value (x, y) at every ten steps\n",
        "- Try out different value for learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeOme5CyG_pI"
      },
      "outputs": [],
      "source": [
        "# plot the graph of the function and the current value (x, y) at every ten steps\n",
        "# try out different value for learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYIpzFf1JGC7"
      },
      "source": [
        "### **Gradient Descent for Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fY0N0cHjJB4I"
      },
      "source": [
        "In OLS, you minimized the sum of squared error (SSE). Here you will be minimizing the cost function. The cost function $J(.)$ is something that evaluates the quality of our result. For our linear regression case; let us take the cost function that is a constant multiple of SSE.\n",
        "\n",
        "\\begin{align*}\n",
        "J(\\beta_0, \\beta_1, \\beta_2, ... ,\\beta_d) &= \\frac{1}{2}\\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2 \\\\\n",
        "&= \\frac{1}{2}\\sum_{i=1}^{n}((\\beta_0x_{i0}+\\beta_1x_{i1} +\\beta_2x_{i2} + ... + \\beta_dx_{id})-y_{i})^2\n",
        "\\end{align*}\n",
        "\n",
        "*d = number of dimension, n = number of samples*\n",
        "\n",
        "In matrix form:\n",
        "\n",
        "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2}\\ \\sum(\\mathbf{X}\\boldsymbol{\\beta} - \\mathbf{y})^2$$\n",
        "\n",
        "*You want to find the parameters $\\beta$ =\n",
        "$\\begin{bmatrix}\n",
        "\\beta_0 \\\\\n",
        "\\beta_1 \\\\\n",
        "\\beta_2 \\\\\n",
        "...   \\\\\n",
        "\\beta_d \\\\\n",
        "\\end{bmatrix} $\n",
        "that minimizes the cost function $J$ using Gradient Descent.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neNmRcOfo6AA"
      },
      "source": [
        "Partial derivative(gradient) of the cost function with respect to $\\beta_1$,\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{\\partial J}{\\partial \\beta_1} &= \\frac{\\partial}{\\partial \\beta_1} ( \\frac{1}{2}\\ \\sum_{i=1}^{n}(\\hat{y_{i}}-{y_{i}})^2)\\\\\n",
        "&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial}{\\partial \\beta_1}(\\hat{y_{i}}-{y_{i}})^2\n",
        "\\end{align*}\n",
        "\n",
        "$\\hspace{8cm}$Applying chain rule,\n",
        "\n",
        "\\begin{align*}\n",
        "\\hspace{8cm}&=\\frac{1}{2}\\ \\sum_{i=1}^{n}\\frac{\\partial (\\hat{y_{i}}-{y_{i}})^2}{\\partial (\\hat{y_{i}}-{y_{i}})} \\times \\frac{\\partial (\\hat{y_{i}}-{y_{i}})}{\\partial \\beta_1}\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times \\frac{\\partial (\\beta_0x_{i0} + \\beta_1x_{i1} + \\beta_2x_{i2} + ... + \\beta_dx_{id}-y_i)}{\\partial \\beta_1}\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i}) \\times x_{i1}\\\\\n",
        "\\therefore \\frac{\\partial J}{\\partial \\beta_1}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i1}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkch2MytqJSU"
      },
      "source": [
        "Similarly,\n",
        "\\begin{align*}\\frac{\\partial J}{\\partial \\beta_0}&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i0}\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\\times 1\\\\\n",
        "&=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\beta_2}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{i2}$$\n",
        "\n",
        "In general,\n",
        "\n",
        "$$\\frac{\\partial J}{\\partial \\beta_d}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{id}$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fusq3_Hcqz6X"
      },
      "source": [
        "We can write this generalized expression in matrix form to calculate the gradients wrt. all the parameters simultaneously as:\n",
        "\n",
        "$$\\frac{\\boldsymbol{\\partial J}}{\\boldsymbol{\\partial \\beta}}\n",
        "= \\begin{bmatrix}\n",
        "\\frac{\\partial J}{\\partial \\beta_0} \\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_1}\\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_2}\\\\\n",
        "... \\\\\n",
        "\\frac{\\partial J}{\\partial \\beta_d}\n",
        "\\end{bmatrix}\n",
        "= \\mathbf{X^T}(\\mathbf{\\hat{y}-y})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C30R4ErurNx7"
      },
      "source": [
        "### Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_tEhHhPrUX6"
      },
      "source": [
        "Find using gradient descent the best fit line for data points $(x1, x2, x3, y) = x1 + 10x2 + 0.005x3 + noise$\n",
        "\n",
        "*Data Creation:*\n",
        " - create 500 sample points\n",
        " - x1 using normal distribution (mean = 2, std = 2.5)\n",
        " - x2 between 0 and 1 (uniform distribution)\n",
        " - x3 between 10,000 and 20,000 (uniform distribution)\n",
        " - noise = random error sampled from normal distribution; mean =0, std = 2\n",
        "\n",
        "*Gradient Descent:*\n",
        " - Take random value of your choice for coefficients to start at\n",
        " - Stop at 100 iterations\n",
        " - Plot on graph how SSE changes with increase in iteration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hlj7BypPsnd-"
      },
      "outputs": [],
      "source": [
        "## gradient descent for multiple linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Okj8xL05smoz"
      },
      "source": [
        "## **Feature scaling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zT47WtTxyH1"
      },
      "source": [
        "Lets take a function $y = 2x_1 + 100x_2 + 5$; what does this tell us?\n",
        "- Increasing x1 by 1 unit increases y by two units and vice-versa\n",
        "- Increasing x2 by two units decreases y by hundred units and vice-versa\n",
        "- Feature $x_2$ is more important than $x_1$\n",
        "\n",
        "But what if we scale each of the values in data points of say feature $x_2$ by 100 times i.e. if $x_2$_new = 100$x_2$ then what will be our new linear regressor? $y = 2x_1 + x_2$_new $+ 5$\n",
        "\n",
        "So, we cannot judge relative importance of values using just the coefficients.\n",
        "\n",
        "Similarly observe the gradient\n",
        "$$\\frac{\\partial J}{\\partial \\beta_d}=\\sum_{i=1}^{n}(\\hat{y_i} - {y_i})x_{id}$$\n",
        "\n",
        "Here, the gradient is greater if we have larger values for feature. So, if there are multiple features and multiple ranges for each, then when taking step using gradient descent we may overshoot or undershoot along some feature axis which can lead to slow convergence of the model.\n",
        "\n",
        "So, we perform feature scaling to scale values of all features within similar range.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqasZiPg1J7S"
      },
      "source": [
        "**There are two important feature scaling methods:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxkfc-vBtvFW"
      },
      "source": [
        "### **Min-Max Scaling:**\n",
        "\n",
        "Min-Max scaling scales the features to have values between 0 and 1.\n",
        "\n",
        "For each value of $i^{th}$ feature, $x_i$, min-max scaling scales the value as:\n",
        "\n",
        "$$\n",
        "x_i = \\frac{x_i - \\text{min}(x_i)}{\\text{max}(x_i) - \\text{min}(x_i)}\n",
        "$$\n",
        "\n",
        "where,\n",
        "\n",
        "$\\text{min}(x_i)$ = minimum of $i^{th}$ feature\n",
        "\n",
        "$\\text{max}(x_i)$ = maximum of $i^{th}$ feature\n",
        "\n",
        "Min-max scaling can be used when you don't have any assumptions about the distribution of data. Algorithms such as decision tree, random forest, Naive Bayes, etc do not assume the data to be normally distributed. However, min-max scaling is highly affected by outliers because of its use of minimum and maximum values. So we prefer standardization over min-max scaling in general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4NNHWDntq2d"
      },
      "source": [
        "### **Standardization:**\n",
        "\n",
        "Standardization scales the features to have values with 0 mean and unit variance.\n",
        "\n",
        "For each value of $i^{th}$ feature, $x_i$, standardization scales the value as:\n",
        "\n",
        "$$\n",
        "x_i = \\frac{x_i - \\mu_i}{\\sigma_i}\n",
        "$$\n",
        "\n",
        "where,\n",
        "\n",
        "$\\mu_i$ = mean of $i^{th}$ feature\n",
        "\n",
        "$\\sigma_i$ = standard deviation of $i^{th}$ feature\n",
        "\n",
        "Standardization is used when you want the data to be normally distributed. Many machine learning algorithms like linear regression, logistic regression and principal component analysis assume the data to be normally distributed. Also, standardization is relatvely less prone to outliers when compared to min-max scaling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyQk6lB63p6D"
      },
      "source": [
        "### Assignment 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9S8hew-4kfb"
      },
      "source": [
        "Using the same data we generated in assignment 2, perform appropriate feature scaling and then fit a regression using gradient descent:\n",
        "- Use same initial value for coefficients you used in assignment 2 but scale appropriately. For example if beta1 was 1 and now you scaled x1 using min-max to be about 4 times as large then use beta1 four times as smaller. *(This is just so that we can compare two scenarios more fairly)*.\n",
        "- Stop at 100 iterations\n",
        "- Plot on graph how SSE changes with increase in iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckEGNOpyxNH3"
      },
      "outputs": [],
      "source": [
        "##standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_r0IAcM7fVV"
      },
      "source": [
        "## **Classification**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq_2td6e7ugR"
      },
      "source": [
        "Till now we studied and solved problems where we wanted to predict continious values but what if our problem was different.\n",
        "\n",
        "For instance given the marks of a student in maths, physics, chemistry will he pass this upcoming assesment or not? How likely?\n",
        "\n",
        "Or given the amount of cholestrol and sugar in blood, is this patient likely to have heart attack in next 5 years? How likely?\n",
        "\n",
        "Note here we have two discre set of output values either 'Yes' or 'No'; numerically lets say 1 or 0. However there is some pattern; For instance the higher the marks the more likely to pass the upcomming assesment and the higher the blood gluose levels the more likely its is to have heart attack. Well logistic regression soles this and we will be looking at it in more detail now."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWhta4mu9fWJ"
      },
      "source": [
        "## **Logistic regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBWjFrbj93ox"
      },
      "source": [
        "Recall the equation of simple linear regression.\n",
        "\n",
        "$$\\hat{y} = \\beta_0 + \\beta_1\\ x$$\n",
        "\n",
        "where  $\\beta_0$ and $\\beta_1$ are the regression coefficients and $x$ is the input feature.\n",
        "\n",
        "In *logistic regression*, we pass the output of the linear regression $\\hat{y}$ to a function known as the sigmoid function. The sigmoid function is of the following form:\n",
        "\n",
        "$$\\sigma(z) = \\frac 1 {(1+ e^{-z})} $$\n",
        "\n",
        "\n",
        "where $z$ is any input real number. So our regression model becomes: $p(x) = \\frac 1 {(1+ e^{-(\\beta_0 + \\beta_1 x)})}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NQkSLj-BOS"
      },
      "source": [
        "### Assignment 4:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhdZdysWIwfw"
      },
      "source": [
        "- First plot the sigmoid function\n",
        " - with a input z (taken from equally spaced points within some range)\n",
        "\n",
        " ---\n",
        "\n",
        "- Next plot the logistic regression for dirrerent values of parameter coefficients\n",
        " - Take a value for $\\beta_0$ and $\\beta_1$\n",
        " - Taking equally spaced values $x$ from certain range find $p(x)$ as given above\n",
        " - Change value of $\\beta_0$ and $\\beta_1$ and note how the output changes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWucY5mVA5Si"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ7GD47n-0PC"
      },
      "source": [
        "**Interpretation of output of logistic regresssion:** *The output of logistic regression, $p(x)$ gives us the probability of the sample belonging to class 1, and $1-p(x)$ gives us the probability of it belonging to class 0.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOeIkUHJCV9w"
      },
      "source": [
        "### Assignment 5:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmwzQ44zIs84"
      },
      "source": [
        "- Lets generate a synthetic data for getting scholarship based on GRE score at a hypothetical university:\n",
        "- A student is likely to get scholarship if his/her GRE score is above 310 and\n",
        "not if less\n",
        "---\n",
        "- Generate the dataset:\n",
        " - Generate a series of 100 random values (feature $x$) for GRE\n",
        " - The associated output $y = 1 \\ if \\ x <= 310 \\ and \\ y = 0 \\ if \\ x > 310$\n",
        " - Randomly invert the $y$ values of few samples (say 5) to simulate real world data\n",
        " - Plot this values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2uYvk9nHG2J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ltLMYjWCJdr"
      },
      "source": [
        "### Cost function for logistic function:\n",
        "\n",
        "The logistic regression model, $ h_w(x) $, can be used to make a prediction as it estimates the probabilities.\n",
        "\n",
        "$$ \\mathcal{\\hat{P}} = h_w(x) = \\sigma(w^Tx) $$\n",
        "\n",
        " We now need to choose the parameters $w$ such that, the model $h_w(x)$ estimates high probabilities for positive class $(y=1)$ and low probabilities for negative class $(y=0)$. The following cost function captures this behavior of $h_w(x)$.\n",
        "\n",
        "<a name='eq1'></a>\n",
        "$$\n",
        "\\text{Cost}(h_w(x),y) =\n",
        "\\begin{cases}\n",
        "  -\\log (h_w(x)) & \\text{if} & y = 1 \\\\\n",
        "  -\\log (1-h_w(x)) & \\text{if} & y = 0\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "This means the model is bad when it gives opposing output to dataset and by larger and larger margin for correspondingly large opposing output. Theoretically if its output is zero when in reality it should have been 1 then it has infinite cost!!\n",
        "\n",
        "*Symbol alert!!: models are often represented in literatures by \"$h()$\" and beta parameters as weight \"$w$\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8p6hat1HRcA"
      },
      "source": [
        "The cost function can be written in a single line as:\n",
        "<a name=\"eq2\"></a>\n",
        "$$ \\text{Cost}(\\hat{y}, y) = J(\\hat{y}) = -y\\log(\\hat{y}) - (1-y)\\log(1-\\hat{y}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4sSYNyuJ8K5"
      },
      "source": [
        "$\\hspace{8cm} where \\ \\hat{y} =  \\beta_0 + \\beta_1 * x; \\ (x, y) \\ is \\ the \\ data \\ point$\n",
        "\n",
        "To take derivative with respect to $\\beta_0$ and $\\beta_1$ we apply chain rule:\n",
        "$ \\frac{\\partial J}{\\partial \\beta} =  \\frac{\\partial J}{\\partial \\hat{y}} \\times \\frac{\\partial \\hat{y}}{\\partial \\beta} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcDmSMsiLX4v"
      },
      "source": [
        "$\\frac{\\partial \\hat{y}}{\\partial \\beta_0} = 1$\n",
        "\n",
        "$\\frac{\\partial \\hat{y}}{\\partial \\beta_1} = x$\n",
        "\n",
        "$\\frac{\\partial J}{\\partial \\hat{y}} = \\hat{y} * (1- \\hat{y})$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdp4WdSnJppg"
      },
      "source": [
        "The above is cost for single data point. For our dataset we sum the errors for all data points. Thus our gradient becomes:\n",
        "\n",
        "$\\hspace{8cm} \\frac{\\partial J}{\\partial \\beta_0} = \\sum_{i=1}^{n} (\\hat{y_i} * (1- \\hat{y_i}) * x_i)$\n",
        "\n",
        "$\\hspace{8cm} where \\ \\hat{y_i} = \\sigma(\\beta_0 + \\beta_1 * x_i)$\n",
        "\n",
        "Similar holds for multivariable case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4rGXyjWIWdy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96rNGIUKIcsH"
      },
      "source": [
        "### Assignment 6:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRT3Is6dJXWw"
      },
      "source": [
        "- Fit logistic regression to data created in assignment 5 and plot the resulting logistic function\n",
        "- Predict output for GRE score 320 (take threshold p = 0.5)\n",
        "- Confirm the prediction using sklearn library\n",
        "\n",
        "\n",
        "*Note:- We may not always take p=0.5 to be threshold depending on application for example if required high confidence for prediction of one class rather than other then we may take other.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT7SZwaszGSv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}